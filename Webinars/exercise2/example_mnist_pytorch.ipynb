{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kreshuklab/teaching-dl-course-2019/blob/master/Webinars/exercise2/example_mnist_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnxLeubl0Ed",
        "colab_type": "text"
      },
      "source": [
        "This is an example Notebook of training a neural network on [MNIST](https://en.wikipedia.org/wiki/MNIST_database) with pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB6Ne9ShlNiL",
        "colab_type": "text"
      },
      "source": [
        "Adapted from the Keras example of Pejman Rasti and the pytorch example of Constantin Pape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68dYc7hHkn5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to show images directly in the notebook\n",
        "%matplotlib inline\n",
        "import numpy as np    # scientific computing \n",
        "import matplotlib.pyplot as plt   # plotting and visualisation\n",
        "# import torch and its libraries\n",
        "import torch  # main torch library for tensor functionality\n",
        "import torch.nn as nn  # layers, activation functions, etc\n",
        "import torch.nn.functional as F  # functions\n",
        "import torch.optim as optim  # optimizers\n",
        "from torchvision import datasets, transforms  # standard datasets, transformations\n",
        "from torch.utils.tensorboard import SummaryWriter # keeping track of training\n",
        "# to display tensorboard directly in the notebook\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5zTGKHenzw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load (downloaded if needed) the MNIST dataset\n",
        "mnist_train = datasets.MNIST('./mnist_data', train=True, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeVZpzrcoJnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot 3 images as gray scale\n",
        "f, axarr = plt.subplots(1, 3)   # three images in a row\n",
        "axarr[0].imshow(np.asarray(mnist_train[0][0]), cmap='gray')\n",
        "axarr[1].imshow(np.asarray(mnist_train[1][0]), cmap='gray')\n",
        "axarr[2].imshow(np.asarray(mnist_train[2][0]), cmap='gray')\n",
        "_ = [ax.axis('off') for ax in axarr]   # remove the axis ticks\n",
        "# show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVRMP7IsSiYT",
        "colab_type": "text"
      },
      "source": [
        "Now we apply some transformations to the dataset.\n",
        "First, we transform data to a `torch.tensor` with `ToTensor` and \n",
        "then normalize the data to have (roughly) zero mean and unit standard deviation with `Normalize`.\n",
        "The second step is a common data normalization in deep learning.\n",
        "\n",
        "In more advanced training pipelines we would normally also apply data augmentation, that we looked through in the first exercise (e.g., rotation, translation, rescaling, affine transformation, etc.)\n",
        "\n",
        "Afterwards we create a dataloader - a class that would fetch a `batch_size` number of images and labels (randomly, if `shuffle=True`) and apply given transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZn8LLpZoARg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the values here represent the mean value (0.1307) and standard deviation (0.3081) of the whole dataset\n",
        "trafos = transforms.Compose([transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# the train loader that fetches data from the 60.000 mnist training examples\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./mnist_data', train=True, download=True, transform=trafos),\n",
        "    batch_size=100, shuffle=True\n",
        ")\n",
        "\n",
        "# the validation loader that fetches data from the 10.000 mnist validation examples\n",
        "val_loader =  torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./mnist_data', train=False, transform=trafos),\n",
        "    batch_size=100, shuffle=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdHntJ0XpG2b",
        "colab_type": "text"
      },
      "source": [
        "Now, we define the main functions that will perform the training and validation loops. Both functions are given the model (= network) and a loader that provides data (input and target). The train function iterates over the training batches and performs gradient descent. For this purpose, it also needs an optimizer, which performs some flavor of (stochastic) gradient descent. The validation function iterates over the validation batches and measures validation loss and accuracy.\n",
        "\n",
        "In general, torch code reads similar to numpy code; instead of np.ndarray the main datastructure is torch.tensor. For debugging, we can simply use prints in the code or set arbitrary breakpoints.\n",
        "\n",
        "Additionally, to log our results in a nice way we would use a tool called [tensorboard](https://www.tensorflow.org/tensorboard). It is developed by TensorFlow, but can be integrated with PyTorch as well. There is a tutorial on what can be visualised with TensorBoard in PyTorch [here](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html). For now we are just interested in tracking our metrics (scalars)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aVhYVBbVSv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loader, optimizer, epoch, log_interval=100, input_is_1d=False, tb_logger=None):\n",
        "    # set model to train mode\n",
        "    model.train()\n",
        "    for batch_id, (x, y) in enumerate(loader):\n",
        "        if input_is_1d:\n",
        "          # if we have a fully connected network\n",
        "          # the input has to be reshaped to be 1d instead of 2d\n",
        "          x = x.view(-1, 784)\n",
        "\n",
        "        # set the gradients to zero, to start with \"clean\" gradients\n",
        "        # in this training iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # apply the model and get our prediction\n",
        "        prediction = model(x)\n",
        "        \n",
        "        # calculate the loss (negative log likelihood loss)\n",
        "        loss = F.nll_loss(prediction, y)\n",
        "        # calculate the gradients (`loss.backward()`) and apply them (`optimizer.ste()`)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # logging\n",
        "        if batch_id % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_id * len(x),\n",
        "                  len(loader.dataset),\n",
        "                  100. * batch_id / len(loader), loss.item()))\n",
        "        \n",
        "        # if we have a valid tb summary writer, we also log the loss there\n",
        "        if tb_logger is not None:\n",
        "            tb_logger.add_scalar(tag='train_loss', # the name of the scalar to log\n",
        "                                 scalar_value=loss.item(), # the value of the scalar at this step\n",
        "                                 # the step number is (epoch number * dataset size + iteration count)\n",
        "                                 global_step=epoch * len(loader) + batch_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pOQCXLeYbpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loader, step=None, input_is_1d=False, tb_logger=None):\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # we don't need gradients during the validation, so we\n",
        "    # disable them via `with torch.no_grad()` to save compute\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            if input_is_1d:\n",
        "              x = x.view(-1, 784)\n",
        "            prediction = model(x)\n",
        "            \n",
        "            # during validation, we sum up the loss of all batches\n",
        "            test_loss += F.nll_loss(prediction, y, reduction='sum').item()\n",
        "            \n",
        "            # we also compute the accuracy. To this end, we compute the\n",
        "            # predictions with highest likelihood and compare with the actual \n",
        "            # target classes\n",
        "            class_pred = prediction.max(1, keepdim=True)[1]\n",
        "            correct += sum(class_pred == y.view_as(class_pred)).item()\n",
        "    # log validation results\n",
        "    test_loss /= len(loader.dataset)\n",
        "    accuracy = 100 * correct / len(loader.dataset)\n",
        "\n",
        "    print('\\nValidate: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "          test_loss, correct, len(loader.dataset), accuracy))\n",
        "    \n",
        "    if tb_logger is not None:\n",
        "        assert step is not None, \"Need to know the current step to log validation results\"\n",
        "        tb_logger.add_scalar(tag='val_loss', scalar_value=test_loss, global_step=step)\n",
        "        tb_logger.add_scalar(tag='val_accuracy', scalar_value=accuracy, global_step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyyjuMjkcf9g",
        "colab_type": "text"
      },
      "source": [
        "Now we will create a [a summary writer](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html#tensorboard-setup) that will write down all the tensorboard logs to a specified folder, and call the tensorboard from inside the Notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGEs1ja_bqAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter('runs/Softmax_model')\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P57Penm7vleq",
        "colab_type": "text"
      },
      "source": [
        "## Linear Softmax Model\n",
        "\n",
        "Next, we define the model / network that we are going to train. Here, we define a single fully connected layer followed by a softmax. We can concatenate the individual layers by `nn.Sequential`, which pipes the output of the previous to the next layer. A fully connected layer is instantiated via `nn.Linear`, which takes the number of input / output channels as first / second argument. In the end, we apply a softmax to output class probabilities.\n",
        "\n",
        "Later, we will see how to define more complex models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQW0aSYlvzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# we use the Adam optimizer that performs a version of stochastic gradient descent\n",
        "# to update the model parameters during training\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O63rodxwIYU",
        "colab_type": "text"
      },
      "source": [
        "And now we come to the actual training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOgcaLEtaq9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we train our model for 5 epochs\n",
        "# 1 epoch consists of iterating once over the training set\n",
        "# and running validation on the complete validation set\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, optimizer, epoch, input_is_1d=True, tb_logger=writer)\n",
        "    # in addition, we need to keep track of the current step for the\n",
        "    # validation tensorboard logs\n",
        "    step = epoch * len(train_loader.dataset)\n",
        "    validate(model, val_loader, step=step, input_is_1d=True, tb_logger=writer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLo--aBtbZEj",
        "colab_type": "text"
      },
      "source": [
        "## Fully Connected Softmax Network\n",
        "\n",
        "In this example, we extend the previous model and train fully connected network with 5 hidden layers. After each layer we use a sigmoid as non-linear activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHr1PvgVbwN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 200),\n",
        "                      nn.Sigmoid(),\n",
        "                      nn.Linear(200, 100),\n",
        "                      nn.Sigmoid(),\n",
        "                      nn.Linear(100, 60),\n",
        "                      nn.Sigmoid(),\n",
        "                      nn.Linear(60, 30),\n",
        "                      nn.Sigmoid(),\n",
        "                      nn.Linear(30, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# we can reuse the same tensorboard session, but we neeb to create a new writer\n",
        "writer = SummaryWriter('runs/FC_Softmax_model')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# train the model for 10 epochs\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, optimizer, epoch,\n",
        "          input_is_1d=True, tb_logger=writer)\n",
        "    step = epoch * len(train_loader.dataset)\n",
        "    validate(model, val_loader, step=step, input_is_1d=True,\n",
        "             tb_logger=writer)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbs46uwpbhfJ",
        "colab_type": "text"
      },
      "source": [
        "## Fully Connected Relu Softmax\n",
        "\n",
        "In this example, we extend the FC_SoftMax Model and use rectified linear units (ReLU) instead of sigmoids as activation functions. ReLUs are simpler activations that return 0 for negative values and the identity for positive values. In practice, this activation yields better performance than sigmoids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP83ePHwdQGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use nn.ReLu instead of nn.Sigmoid\n",
        "model = nn.Sequential(nn.Linear(784, 200),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(200, 100),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(100, 60),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(60, 30),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(30, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "writer = SummaryWriter('runs/FC_Relu_Softmax_model')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# train the model for 10 epochs\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, optimizer, epoch,\n",
        "          input_is_1d=True, tb_logger=writer)\n",
        "    step = epoch * len(train_loader.dataset)\n",
        "    validate(model, val_loader, step=step, input_is_1d=True,\n",
        "             tb_logger=writer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyb3AxvIdVzx",
        "colab_type": "text"
      },
      "source": [
        "## FC_Relu_Dropout_SoftMax\n",
        "\n",
        "In this example, we extend FC_ReLU_SoftMax Model and use [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXHxO4ixdkw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the probability to drop neurons with dropout\n",
        "p_drop = .2\n",
        "# use nn.Dropout after relu activations\n",
        "model = nn.Sequential(nn.Linear(784, 200),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(p=p_drop),\n",
        "                      nn.Linear(200, 100),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(p=p_drop),\n",
        "                      nn.Linear(100, 60),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(p=p_drop),\n",
        "                      nn.Linear(60, 30),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(p=p_drop),\n",
        "                      nn.Linear(30, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "writer = SummaryWriter('runs/FC_Relu_Dropout_Softmax_model')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# train the model for 10 epochs\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, optimizer, epoch,\n",
        "          input_is_1d=True, tb_logger=writer)\n",
        "    step = epoch * len(train_loader.dataset)\n",
        "    validate(model, val_loader, step=step, input_is_1d=True,\n",
        "             tb_logger=writer)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTHO2Eo5eH2O",
        "colab_type": "text"
      },
      "source": [
        "## CNN Relu Dropout SoftMax\n",
        "\n",
        "In this example, we will use a [Convolutional](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) Network instead of a fully connected network.\n",
        "\n",
        "For more complex models, we stop using nn.Sequential. Instead, we implement models via classes that inherit from nn.Module and implement a forward method, in which the sub-modules are applied.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceQXpOUNeZSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # we need to call the super constructor before adding any\n",
        "        # nn.modules as members\n",
        "        super().__init__()\n",
        "        # add the convolutional block\n",
        "        self.convs = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2),\n",
        "                                   nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Conv2d(in_channels=8, out_channels=12, kernel_size=3))\n",
        "        # add the fully connected block\n",
        "        self.fc = nn.Sequential(nn.Linear(972, 200),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(0.2),\n",
        "                                nn.Linear(200, 10),\n",
        "                                nn.LogSoftmax(dim=1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.convs(input)\n",
        "        # convs returns outpu of size (batches, 12, 9, 9)\n",
        "        # to feed it into the fully conneted layer, we need to reshape\n",
        "        # to 1d input (batchess, 12 * 9 * 9 = 972)\n",
        "        x = x.view(-1, 972)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "model = ConvNet()\n",
        "\n",
        "writer = SummaryWriter('runs/CNN_Relu_Dropout_Softmax_model')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# train otheur model for 10 epochs\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, optimizer, epoch,\n",
        "          tb_logger=writer)\n",
        "    step = epoch * len(train_loader.dataset)\n",
        "    validate(model, val_loader, step=step,\n",
        "             tb_logger=writer)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}